{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from time import sleep\n",
    "thet_l = []\n",
    "mistake = []\n",
    "# m denotes the number of examples here, not the number of features\n",
    "def gradientDescent(x, y, theta, alpha, m, numIterations):\n",
    "    global thet_l, mistake\n",
    "    xTrans = x.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        hypothesis = np.dot(x, theta)\n",
    "        loss = hypothesis - y\n",
    "        # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "        # But to be consistent with the gradient, I include it)\n",
    "        cost = np.sum(loss ** 2) / (2 * m)\n",
    "        mistake.append(cost)\n",
    "        #print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        # avg gradient per example\n",
    "        gradient = np.dot(xTrans, loss) / m\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "        thet_l.append(theta)\n",
    "    return theta\n",
    "\n",
    "\n",
    "def genData(numPoints, bias, variance):\n",
    "    X = np.zeros(shape=numPoints)\n",
    "    Y = np.zeros(shape=numPoints)\n",
    "    for i in range(0, numPoints):\n",
    "        # bias feature\n",
    "        X[i] = i\n",
    "        # our target variable\n",
    "        Y[i] = (i + bias) + random.uniform(0, 1) * variance\n",
    "    return X, Y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = genData(100, 25, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    " \n",
    "coefs = []\n",
    "losses = []\n",
    "    \n",
    "def local_grad(x, y, a, b):\n",
    "    return 2 * x ** 2 * a + 2 * x * b - 2 * x * y, 2 * b + 2 * x * a - 2 * y\n",
    " \n",
    "def grad(X, Y, a, b):\n",
    "    gr_a = 0\n",
    "    gr_b = 0\n",
    "    for i in range(len(X)):\n",
    "        gr = local_grad(X[i], Y[i], a, b)\n",
    "        gr_a += gr[0]\n",
    "        gr_b += gr[1]\n",
    "    mdl = (gr_a ** 2 + gr_b ** 2) ** (1 / 2)\n",
    "    gr_a /= mdl\n",
    "    gr_b /= mdl\n",
    "    return gr_a, gr_b\n",
    " \n",
    "def loss(X, Y, a, b):\n",
    "    loss = 0\n",
    "    for i in range(len(X)):\n",
    "        loss += (a * X[i] + b - Y[i]) ** 2\n",
    "    return loss\n",
    " \n",
    "def local_min_step(x, y, a, b, gr_a, gr_b):\n",
    "    k = gr_a * x + gr_b\n",
    "    m =  a * x + b - y\n",
    "    lambda_coef = k ** 2\n",
    "    free_coef = k * m\n",
    "    return lambda_coef, free_coef\n",
    " \n",
    "def min_step(X, Y, a, b, gr_a, gr_b):\n",
    "    lambda_coef = 0\n",
    "    free_coef = 0\n",
    "    for i in range(len(X)):\n",
    "        coef = local_min_step(X[i], Y[i], a, b, gr_a, gr_b)\n",
    "        lambda_coef += coef[0]\n",
    "        free_coef += coef[1]\n",
    "    return -free_coef / lambda_coef\n",
    " \n",
    "def GDS(X, Y, iter):\n",
    "    global coefs, losses\n",
    "    coefs = []\n",
    "    losses = []\n",
    "    a = 1\n",
    "    b = 0\n",
    "    grad_a, grad_b = 1, 1\n",
    "    lambda_step = 1\n",
    "    a_best, b_best = a, b\n",
    "    best = loss(X, Y, a, b)\n",
    "    num = [i for i in range(len(X))]\n",
    "    random.shuffle(num)\n",
    "    #print(num)\n",
    "    i = 0\n",
    "    C = len(X)\n",
    "    coefs.append((a, b))\n",
    "    losses.append(best)\n",
    "    for k in range(iter):\n",
    "        #print(loss(X, Y, a, b))\n",
    "        if (best > loss(X, Y, a, b)):\n",
    "            best = loss(X, Y, a, b)\n",
    "            a_best, b_best = a, b\n",
    "        #print(a, b)\n",
    "        grad_a, grad_b = local_grad(X[i % len(X)], Y[i % len(X)], a, b)\n",
    "        mdl = (grad_a ** 2 + grad_b ** 2) ** (1 / 2)\n",
    "        if (mdl == 0):\n",
    "            continue        \n",
    "        grad_a /= mdl\n",
    "        grad_b /= mdl\n",
    "        #print(grad_a, grad_b)\n",
    "        coef = local_min_step(X[i % len(X)], Y[i % len(X)], a, b, -grad_a, -grad_b)\n",
    "        if (coef[0] == 0):\n",
    "            continue\n",
    "        lambda_step = -coef[1] / coef[0]\n",
    "        C = 1 / (i + 1) ** (3 / 5)\n",
    "        a -= lambda_step * C * grad_a\n",
    "        b -= lambda_step * C * grad_b\n",
    "        coefs.append((a, b))\n",
    "        losses.append(loss(X, Y, a, b))\n",
    "        i += 1\n",
    "    #print(a_best, b_best)\n",
    "    #print(best)\n",
    "    return a_best, b_best\n",
    " \n",
    "def GD(X, Y, iter):\n",
    "    global coefs, losses\n",
    "    coefs = []\n",
    "    losses = []\n",
    "    a = 1\n",
    "    b = 0\n",
    "    grad_a, grad_b = 1, 1\n",
    "    lambda_step = 1\n",
    "    coefs.append((a, b))\n",
    "    losses.append(loss(X, Y, a, b))\n",
    "    for k in range(iter):\n",
    "        #print(loss(X, Y, a, b))\n",
    "        #print(a, b)\n",
    "        grad_a, grad_b = grad(X, Y, a, b)\n",
    "        #print(grad_a, grad_b)\n",
    "        lambda_step = min_step(X, Y, a, b, -grad_a, -grad_b)\n",
    "        a -= lambda_step * grad_a\n",
    "        b -= lambda_step * grad_b\n",
    "        coefs.append((a, b))\n",
    "        losses.append(loss(X, Y, a, b))\n",
    "    #print(a, b)\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.99385811093545451, 29.982462197358526)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter = 10000\n",
    "GD(X, Y, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 62, 7, 6, 36, 28, 68, 20, 55, 61, 54, 93, 77, 70, 19, 88, 52, 48, 60, 41, 49, 30, 51, 42, 23, 85, 27, 94, 3, 10, 89, 57, 79, 22, 2, 38, 90, 35, 8, 75, 11, 99, 37, 95, 84, 78, 50, 40, 17, 43, 5, 39, 53, 14, 67, 58, 81, 63, 56, 34, 26, 98, 18, 72, 4, 15, 0, 92, 82, 9, 47, 86, 97, 31, 21, 71, 29, 66, 64, 25, 74, 1, 91, 87, 45, 46, 13, 80, 69, 76, 33, 96, 16, 83, 59, 44, 32, 65, 24, 12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 30.665966141759444)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter = 100\n",
    "GDS(X, Y, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2,1,2)\n",
    "ax2 = fig.add_subplot(2,1,1)\n",
    "ax1.plot(X,Y, 'ro')\n",
    "def animate(i):\n",
    "    ax1.clear()\n",
    "    ax2.plot(i, losses[i], 'go')\n",
    "    ax1.text(80, 20,str(i))\n",
    "    ax1.plot(X, Y, 'ro')\n",
    "    ax1.plot(X, coefs[i][0]*X+coefs[i][1], 'g')\n",
    "    plt.draw()\n",
    "ani = animation.FuncAnimation(fig, animate, interval=10000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
